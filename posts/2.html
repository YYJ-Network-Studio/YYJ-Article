<!DOCTYPE html>
<head>
    <!--YYJ Network Studio Website-->
	<title>AI人工智慧 #2｜神經網路(Neural Network)與激活函數(Activation Function)</title>
	<!--meta > charset-->
	<meta charset="utf-8">
	<!--meta > http-equiv-->
	<meta http-equiv="content-language" content="zh-TW">
	<!--meta > name-->
	<meta name="author" content="YYJ">
	<meta name="distribution" content="Taiwan">
	<meta name="revisit-after" content="3 days">
	<meta name="copyright" content="2023 © YYJ Network Studio">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<!-- Keywords -->
	<meta name="keywords" Lang="zh-TW" content="人工智慧,機器學習,深度學習,神經網路,神經網路結構,激活函數">
	<meta name="keywords" Lang="EN" content="AI,ML,DL,NN,Neural Network,Activation Function,Sigmoid,ReLU,Leaky ReLU">
	<!-- Description -->
	<meta name="description" content="神經網路的三層結構以及激活函數(Sigmoid, ReLU, Leaky ReLU)是什麼？">
	<!--link > rel-->
	<link rel="icon" href="../static/images/favicon.ico">
	<link rel="stylesheet" type="text/css" href="../static/css/css.css">
	<link rel="stylesheet" type="text/css" href="../static/css/rwd.css">
	<link rel="stylesheet" type="text/css" href="../static/css/article.css">
	<link rel="stylesheet" type="text/css" href="../static/css/bootstrap.min.css">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
</head>
<body>
	<nav>
		<div class="logo">
			YYJ Network Studio
		</div>
		<div class="i">
			<i class="fa-regular fa-moon" style="color: white;" id="theme"></i>
		</div>
		<input type="checkbox" id="click">
    	<label for="click" class="menu">
        	<i class="fas fa-bars"></i>
    	</label>
		<ul>
			<li>
				<a href="https://yyjstudio.com/index">首頁</a>
            	<a class="active" href="../index.html">文章</a>
            	<a href="https://yyjstudio.com/service">服務</a>
			</li>
		</ul>
	</nav>
	<h1>AI人工智慧 #2｜神經網路(Neural Network)與激活函數(Activation Function)</h1>
	<div class="article-line"></div>
	<div class="container">
		<div class="row">
			<div class="col-12" style="text-align: left;">
				<img src="https://www.tibco.com/sites/tibco/files/media_entity/2021-05/neutral-network-diagram.svg" alt="神經網路圖">
				<p class="source">資料來源：https://www.tibco.com</p>
				<h3>神經網路的三層</h3>
				<p>再來複習一下神經網路的大致結構，分別是：</p>
				<p>輸入層(Input Layer)：接受非線性(Non-Linearity)輸入訊息，輸入的訊息稱為輸入向量，也可以稱為自變量(Independent Variable)。</p>
				<p>隱藏層(Hidden Layer)：輸入層和輸出層中間的一層(也可以是多層)，隱藏層的神經元(Neuron)負責計算處理資料，隱藏層的數量越多，計算的複雜度和時間也會隨之增加。</p>
				<p>輸出層(Output Layer)：神經網路的最後一層，輸出層的神經元數量取決於會有哪些輸出結果。</p>
				<h3>激活函數(Activation Function)</h3>
				<p>激活函數在一層神經元輸出和下一層神經元輸入之間，每個神經元輸出都會經過激活函數，激活函數會將線性(Linearity)轉換成非線性(Non-Linearity)。</p>
				<h4>為什麼需要將線性轉換成非線性？</h4>
				<p>簡單來講，如果使用線性函數，無論神經網路有多少層(Layer)，都能用一層取代，也就是說非線性是為了讓神經網路可以有更複雜的計算。</p>
				<h4>激活函數Sigmoid, ReLU, Leaky ReLU</h4>
				<h5>Sigmoid</h5>
				<p>中文稱「S形函數」，是將輸入數值映射到0到1之間，可以把下圖的橫線稱作x，直線稱作y，Sigmoid是將橫線x輸入值，轉換成直線y輸出值0-1之間。y輸出值趨近於0時，線條的斜率會變得更小更平坦，也就是梯度變小導致梯度消失(Vanishing Gradient)的問題發生，以下提到的ReLU激活函數，就能解決這個問題。</p>
				<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/faaa0c014ae28ac67db5c49b3f3e8b08415a3f2b" alt="Sigmoid公式" width="50px" height="50px">
				<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png" alt="Sigmoid圖形">
				<p class="source">資料來源：https://wikimedia.org</p>
				<h5>ReLU</h5>
				<p>全名為Rectified Linear Unit，中文稱「線性整流函數」，計算方式簡單所以速度比Sigmoid，還能避免梯度消失和梯度爆炸(Exploding Gradients)的問題。</p>
				<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5fa5d3598751091eed580bd9dca873f496a2d0ac" alt="ReLU公式" width="30px" height="30px">
				<p class="source">資料來源：https://wikimedia.org</p>
				<br>f(x)輸入值如果大於0，則輸出x，x = 1，f(x) = (0,1) = 1
				<br>f(x)輸入值如果小於等於0，則輸出0，x = -1，f(x) = (0,-1) = 0
				<img src="https://upload.wikimedia.org/wikipedia/commons/c/c9/Ramp_function.svg" alt="ReLU圖形">
				<p class="source">資料來源：https://wikimedia.org</p>
				<h5>Leaky ReLU</h5>
				<p>中文稱「帶泄露線性整流」，是ReLU的進化版，但不代表說能取代ReLU。如果輸入為負數，則輸出為0，這導致神經元不會激活，Leaky ReLU會以很小的值(如0.01)來將負數的x輸入值，轉換成y輸出值。</p>
				<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7ef462b36056ff49700914fc305a39cd0d8c1ef1" alt="Leaky ReLU公式" width="50px" height="50px">
				<p class="source">資料來源：https://wikimedia.org</p>
				<img src="http://kwassistfile.cupoy.com/0000017BAA952F3A000000096375706F795F72656C65617365414E53/1630034429718/large" alt="Leaky ReLU圖形">
				<p class="source">資料來源：http://kwassistfile.cupoy.com</p>
				<h4>重點整理</h4>
				<h5>Sigmoid</h5>
				<p>優點：將輸出標準化，容易理解</p>
				<p>缺點：正負輸入值如果非常廣，輸出的梯度會趨近於0時，導致梯度消失的問題</p>
				<h5>ReLU</h5>
				<p>優點：計算速度比Sigmoid快，解決了梯度消失的問題</p>
				<p>缺點：不適用輸入值為負數，神經元死亡的問題，當輸入接近零或負數時，梯度會變0，導致無法反向傳播(Backpropagation)。</p>
				<h5>Leaky ReLU</h5>
				<p>優點：可應用於輸入值為負數，解決了ReLU梯度死亡的問題</p>
				<p>缺點：正負結果不一致(正負公式不同)，需要選擇適當的負數斜率才能達到最佳計算效果</p>
				<p>還有非常多種激活函數，這次就只講Sigmoid, ReLU, Leaky ReLU，目前市面上主流的激活函數是ReLU，當然還是要根據用途選擇適合的激活函數。</p>
			</div>
		</div>
	</div>
	<footer>
		<div class="footer-social-media">
			<a href="https://www.facebook.com/profile.php?id=100064249505130"><i class="fa-brands fa-facebook"></i></a>
			<a href="https://twitter.com/yyj_network"><i class="fa-brands fa-twitter"></i></a>
			<a href="https://www.instagram.com/yang.yaozhu"><i class="fa-brands fa-instagram"></i></a>
			<a href="https://github.com/YYJ-TW"><i class="fa-brands fa-github"></i></a>
		</div>
		<div class="footer-pages">
			<a href="https://yyjstudio.com/terms">服務條款</a>
			<a href="https://yyjstudio.com/job">職缺</a>
		</div>
		<div class="footer-copyright">
			<p>2023 &copy; YYJ Network Studio</p>
		</div>
	</footer>
	<script src="../static/js/theme.js"></script>
	<script src="https://kit.fontawesome.com/098165415c.js" crossorigin="anonymous"></script>
</body>